import numpy as np
import torch
from models.language_encoder import load_model as load_text_model, get_embeddings as get_text_embeddings
from models.image_encoder import get_image_embeddings
from models.audio_encoder import get_audio_embeddings
from models.multimodal_encoder import get_multimodal_embeddings
from brain_mapping.encoding_model import evaluate_layers_cv, get_best_brain_score
from brain_mapping.roi_score import compute_model_fit_all_rois

def compute_brain_alignment(model_name, modality, inputs, y, roi_dict, device="cpu", alpha=1.0, n_splits=3, batch_size=8):
    """
    é€šç”¨è„‘-æ¨¡å‹å¯¹é½å‡½æ•°
    æ”¯æŒ text / image / audio / multimodal æ¨¡å‹
    
    å‚æ•°ï¼š
        model_name : str
            æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ "bert-base-uncased" / "facebook/wav2vec2-base" / "openai/clip-vit-base-patch16"
        modality : str
            æ¨¡å‹ç±»å‹ï¼Œå–å€¼ä¸º "text" / "image" / "audio" / "multimodal"
        inputs : dict
            æ¨¡æ€è¾“å…¥ï¼š
              - text æ¨¡å‹: {"texts": [...]}  
              - image æ¨¡å‹: {"image_paths": [...]}  
              - audio æ¨¡å‹: {"audio_paths": [...]}  
              - multimodal æ¨¡å‹: {"texts": [...], "image_paths": [...]}  
        y : np.ndarray
            è„‘ä¿¡å· (TÃ—V)
        roi_dict : dict
            ROI åç§° â†’ voxel ç´¢å¼•
        device : str
            "cpu" / "cuda" / "mps"
        alpha : float
            å²­å›å½’æƒ©ç½šå‚æ•°
        n_splits : int
            äº¤å‰éªŒè¯æŠ˜æ•°
        batch_size : int
            æ‰¹å¤„ç†å¤§å°

    è¿”å›ï¼š
        best_layer, roi_vector, roi_scores
    """
    print(f"\nğŸ§  Processing {modality} model: {model_name}")

    # ===== Step 1: åŠ è½½æ¨¡å‹å¹¶æå– embedding =====
    if modality == "text":
        tokenizer, model = load_text_model(model_name, device=device)
        X_layers = get_text_embeddings(tokenizer, model, inputs["texts"], device=device, all_layers=True, cls_only=True, batch_size=batch_size)

    elif modality == "image":
        from transformers import AutoImageProcessor, AutoModel
        processor = AutoImageProcessor.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name, output_hidden_states=True).to(device)
        X_layers = get_image_embeddings(processor, model, inputs["image_paths"], device=device, all_layers=True, cls_only=True)

    elif modality == "audio":
        from transformers import AutoProcessor, AutoModel
        processor = AutoProcessor.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name, output_hidden_states=True).to(device)
        X_layers = get_audio_embeddings(processor, model, inputs["audio_paths"], device=device, all_layers=True)

    elif modality == "multimodal":
        from transformers import AutoProcessor, AutoModel
        processor = AutoProcessor.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name).to(device)
        text_embeds, image_embeds, joint_embeds = get_multimodal_embeddings(processor, model, 
                                                                           inputs["texts"], inputs["image_paths"], device=device)
        X_layers = [joint_embeds]  # åªæœ‰ä¸€å±‚èåˆ embedding

    else:
        raise ValueError(f"Unsupported modality: {modality}")

    # ===== Step 2: è®¡ç®— brain score =====
    scores = evaluate_layers_cv(X_layers, y, alpha=alpha, n_splits=n_splits, device=device)
    best_layer, best_score = get_best_brain_score(scores)
    print(f"âœ… Best layer for {model_name}: {best_layer} (mean corr = {best_score:.4f})")

    # ===== Step 3: å–æœ€ä½³å±‚ embedding =====
    X_best = X_layers[best_layer]

    # ===== Step 4: ROI æ‹Ÿåˆ =====
    roi_vector, roi_scores = compute_model_fit_all_rois(X_best, y, roi_dict, alpha=alpha)
    print(f"ğŸ Finished {model_name}")

    for roi, score in roi_scores.items():
        print(f"   ROI {roi}: {score:.4f}")

    return best_layer, roi_vector, roi_scores

